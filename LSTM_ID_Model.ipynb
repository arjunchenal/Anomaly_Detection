{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ID ALONE MODEL\n",
    "# This model is used to predict the next ID of a sequence of IDs\n",
    "# The model is trained on the ID alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libraries.utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal base path: ./trace_data/theft_protection/single_thread/version_3/normal\n",
      "Faulty base path: ./trace_data/theft_protection/single_thread/version_3/faulty_data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 3                                 ### format of data collection\n",
    "\n",
    "base_dir = './trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varlist 1 is consistent with varlist 0\n",
      "varlist 2 is consistent with varlist 0\n",
      "varlist 3 is consistent with varlist 0\n"
     ]
    }
   ],
   "source": [
    "# Check consistency\n",
    "if VER == 3:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]] + varlist_path)\n",
    "    if check_con:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15], [13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14], [9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10], [7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list):\n",
    "            id_sequence = [int(trace[0]) for trace in traces]\n",
    "            data.append(id_sequence)\n",
    "    return data\n",
    "\n",
    "train_data = load_data(train_data_path)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: shape = (110,)\n",
      "Dataset 2: shape = (750,)\n",
      "Dataset 3: shape = (750,)\n",
      "Dataset 4: shape = (250,)\n"
     ]
    }
   ],
   "source": [
    "for idx, td in enumerate(train_data):\n",
    "    td_array = np.array(td)  \n",
    "    print(f\"Dataset {idx + 1}: shape = {td_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare LSTM training data\n",
    "sequence_length = 10       # Already tuned parameter value\n",
    "X_train, y_train = [], []\n",
    "for single_file_data in train_data:\n",
    "    for i in range(len(single_file_data) - sequence_length):\n",
    "        X_train.append(single_file_data[i:i + sequence_length])\n",
    "        y_train.append(single_file_data[i + sequence_length])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "X_train_new = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_new = X_val.reshape(-1, X_val.shape[-1])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_new)\n",
    "X_val_scaled = scaler.transform(X_val_new)\n",
    "\n",
    "X_train = X_train_scaled.reshape(X_train.shape)\n",
    "X_val = X_val_scaled.reshape(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 75.4798 - mae: 8.2366 - val_loss: 13.3546 - val_mae: 2.9037\n",
      "Epoch 2/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 13.7704 - mae: 3.0169 - val_loss: 8.0947 - val_mae: 2.1940\n",
      "Epoch 3/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 8.5966 - mae: 2.3655 - val_loss: 6.4275 - val_mae: 2.0787\n",
      "Epoch 4/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.2459 - mae: 2.1535 - val_loss: 5.6577 - val_mae: 1.7366\n",
      "Epoch 5/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 7.1457 - mae: 2.0956 - val_loss: 5.5791 - val_mae: 1.8423\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.8434 - mae: 2.0291 - val_loss: 5.4945 - val_mae: 1.8068\n",
      "Epoch 7/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.9244 - mae: 1.9967 - val_loss: 5.4673 - val_mae: 1.7295\n",
      "Epoch 8/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 7.2996 - mae: 2.0590 - val_loss: 5.5250 - val_mae: 1.8458\n",
      "Epoch 9/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.3517 - mae: 1.9588 - val_loss: 5.4198 - val_mae: 1.7871\n",
      "Epoch 10/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.3809 - mae: 1.9749 - val_loss: 5.8504 - val_mae: 1.9583\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.9208 - mae: 2.0613 - val_loss: 5.3821 - val_mae: 1.7557\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 7.0668 - mae: 2.1028 - val_loss: 5.3635 - val_mae: 1.7717\n",
      "Epoch 13/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.5973 - mae: 1.9460 - val_loss: 5.6594 - val_mae: 1.8877\n",
      "Epoch 14/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.2434 - mae: 1.9415 - val_loss: 5.3840 - val_mae: 1.7705\n",
      "Epoch 15/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.2298 - mae: 1.9125 - val_loss: 5.3528 - val_mae: 1.7578\n",
      "Epoch 16/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.5446 - mae: 1.9582 - val_loss: 5.4894 - val_mae: 1.8387\n",
      "Epoch 17/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.5837 - mae: 1.9893 - val_loss: 5.4506 - val_mae: 1.8285\n",
      "Epoch 18/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.6882 - mae: 1.9857 - val_loss: 5.4906 - val_mae: 1.6651\n",
      "Epoch 19/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.2314 - mae: 1.9073 - val_loss: 5.4070 - val_mae: 1.8140\n",
      "Epoch 20/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6.4698 - mae: 1.9677 - val_loss: 5.3293 - val_mae: 1.7734\n",
      "Epoch 21/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.9689 - mae: 1.8531 - val_loss: 5.3877 - val_mae: 1.8082\n",
      "Epoch 22/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.1508 - mae: 1.8915 - val_loss: 5.7243 - val_mae: 1.9209\n",
      "Epoch 23/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.7514 - mae: 2.0333 - val_loss: 5.3365 - val_mae: 1.6952\n",
      "Epoch 24/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.7938 - mae: 2.0129 - val_loss: 6.0623 - val_mae: 2.0563\n",
      "Epoch 25/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.7032 - mae: 2.0287 - val_loss: 6.1610 - val_mae: 2.0905\n",
      "Epoch 26/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.6237 - mae: 2.0407 - val_loss: 5.3882 - val_mae: 1.8113\n",
      "Epoch 27/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.4552 - mae: 1.9620 - val_loss: 5.6447 - val_mae: 1.8972\n",
      "Epoch 28/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.9105 - mae: 1.8517 - val_loss: 5.6307 - val_mae: 1.8997\n",
      "Epoch 29/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.7042 - mae: 2.0130 - val_loss: 5.2409 - val_mae: 1.7376\n",
      "Epoch 30/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 6.0550 - mae: 1.9046 - val_loss: 5.1710 - val_mae: 1.7362\n",
      "Epoch 31/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.3637 - mae: 1.9410 - val_loss: 5.9895 - val_mae: 2.0738\n",
      "Epoch 32/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.9673 - mae: 1.9283 - val_loss: 5.2589 - val_mae: 1.8442\n",
      "Epoch 33/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 5.8119 - mae: 1.8586 - val_loss: 5.0195 - val_mae: 1.6404\n",
      "Epoch 34/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.6840 - mae: 2.0417 - val_loss: 5.0340 - val_mae: 1.6898\n",
      "Epoch 35/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.4708 - mae: 1.9434 - val_loss: 5.1242 - val_mae: 1.7778\n",
      "Epoch 36/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.2680 - mae: 1.8887 - val_loss: 5.3544 - val_mae: 1.9139\n",
      "Epoch 37/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.0347 - mae: 1.9124 - val_loss: 4.9986 - val_mae: 1.7128\n",
      "Epoch 38/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.2403 - mae: 1.9417 - val_loss: 5.0287 - val_mae: 1.7418\n",
      "Epoch 39/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.8115 - mae: 1.8180 - val_loss: 4.9726 - val_mae: 1.6555\n",
      "Epoch 40/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.8297 - mae: 1.8340 - val_loss: 5.3746 - val_mae: 1.9252\n",
      "Epoch 41/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 6.0206 - mae: 1.9275 - val_loss: 4.8761 - val_mae: 1.6219\n",
      "Epoch 42/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.9341 - mae: 1.8703 - val_loss: 4.8828 - val_mae: 1.6895\n",
      "Epoch 43/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 6.0252 - mae: 1.8743 - val_loss: 4.7981 - val_mae: 1.6086\n",
      "Epoch 44/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 5.7190 - mae: 1.8597 - val_loss: 5.0398 - val_mae: 1.8215\n",
      "Epoch 45/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.5982 - mae: 1.8490 - val_loss: 5.1064 - val_mae: 1.8757\n",
      "Epoch 46/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 5.4800 - mae: 1.8763 - val_loss: 4.2318 - val_mae: 1.6598\n",
      "Epoch 47/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.8967 - mae: 1.7736 - val_loss: 3.9580 - val_mae: 1.4944\n",
      "Epoch 48/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.4419 - mae: 1.6485 - val_loss: 3.3492 - val_mae: 1.3233\n",
      "Epoch 49/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 4.3140 - mae: 1.6885 - val_loss: 4.5135 - val_mae: 1.6776\n",
      "Epoch 50/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 4.4396 - mae: 1.6870 - val_loss: 3.2543 - val_mae: 1.4018\n",
      "Epoch 51/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.5982 - mae: 1.4904 - val_loss: 3.0368 - val_mae: 1.3549\n",
      "Epoch 52/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.2236 - mae: 1.3982 - val_loss: 3.0539 - val_mae: 1.4013\n",
      "Epoch 53/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.5692 - mae: 1.4895 - val_loss: 2.6183 - val_mae: 1.1912\n",
      "Epoch 54/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 3.0120 - mae: 1.3417 - val_loss: 2.6306 - val_mae: 1.1604\n",
      "Epoch 55/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.9627 - mae: 1.3346 - val_loss: 3.3688 - val_mae: 1.4409\n",
      "Epoch 56/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 3.2374 - mae: 1.3971 - val_loss: 2.4504 - val_mae: 1.1529\n",
      "Epoch 57/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.7027 - mae: 1.2349 - val_loss: 3.5779 - val_mae: 1.4986\n",
      "Epoch 58/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.9358 - mae: 1.3282 - val_loss: 2.9252 - val_mae: 1.3438\n",
      "Epoch 59/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.9142 - mae: 1.3400 - val_loss: 2.8721 - val_mae: 1.3493\n",
      "Epoch 60/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.6395 - mae: 1.2518 - val_loss: 2.6804 - val_mae: 1.2447\n",
      "Epoch 61/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.5651 - mae: 1.2131 - val_loss: 2.2429 - val_mae: 1.0986\n",
      "Epoch 62/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.5184 - mae: 1.2197 - val_loss: 2.3069 - val_mae: 1.1551\n",
      "Epoch 63/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.6284 - mae: 1.2617 - val_loss: 2.0697 - val_mae: 1.0398\n",
      "Epoch 64/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.3239 - mae: 1.1545 - val_loss: 1.9429 - val_mae: 1.0101\n",
      "Epoch 65/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.0927 - mae: 1.0771 - val_loss: 2.3801 - val_mae: 1.1537\n",
      "Epoch 66/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.2860 - mae: 1.1401 - val_loss: 1.9363 - val_mae: 1.0699\n",
      "Epoch 67/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 2.0734 - mae: 1.0741 - val_loss: 1.7708 - val_mae: 0.9512\n",
      "Epoch 68/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9699 - mae: 1.0353 - val_loss: 1.9200 - val_mae: 1.1024\n",
      "Epoch 69/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.1037 - mae: 1.0885 - val_loss: 1.7283 - val_mae: 0.9731\n",
      "Epoch 70/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.9579 - mae: 1.0332 - val_loss: 1.7722 - val_mae: 0.9936\n",
      "Epoch 71/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 2.1522 - mae: 1.1118 - val_loss: 1.8291 - val_mae: 1.0024\n",
      "Epoch 72/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.1545 - mae: 1.1207 - val_loss: 1.7035 - val_mae: 0.9882\n",
      "Epoch 73/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9154 - mae: 1.0556 - val_loss: 1.7354 - val_mae: 0.9747\n",
      "Epoch 74/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.9543 - mae: 1.0321 - val_loss: 1.7413 - val_mae: 0.9386\n",
      "Epoch 75/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 2.0034 - mae: 1.0456 - val_loss: 1.6008 - val_mae: 0.8832\n",
      "Epoch 76/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.7160 - mae: 0.9683 - val_loss: 1.6191 - val_mae: 0.9302\n",
      "Epoch 77/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 1.7984 - mae: 0.9631 - val_loss: 1.6211 - val_mae: 0.8951\n",
      "Epoch 78/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.6510 - mae: 0.9154 - val_loss: 1.5926 - val_mae: 0.8948\n",
      "Epoch 79/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.7426 - mae: 0.9448 - val_loss: 1.6004 - val_mae: 0.8696\n",
      "Epoch 80/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.7464 - mae: 0.9388 - val_loss: 1.7414 - val_mae: 0.9825\n",
      "Epoch 81/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.7725 - mae: 0.9867 - val_loss: 1.7023 - val_mae: 0.9758\n",
      "Epoch 82/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9667 - mae: 1.0446 - val_loss: 1.7217 - val_mae: 0.9732\n",
      "Epoch 83/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9845 - mae: 1.0335 - val_loss: 1.6084 - val_mae: 0.8710\n",
      "Epoch 84/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5943 - mae: 0.8966 - val_loss: 1.5550 - val_mae: 0.8770\n",
      "Epoch 85/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.7157 - mae: 0.9557 - val_loss: 2.2934 - val_mae: 1.2140\n",
      "Epoch 86/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9836 - mae: 1.0517 - val_loss: 1.5300 - val_mae: 0.8668\n",
      "Epoch 87/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6393 - mae: 0.9123 - val_loss: 1.5487 - val_mae: 0.8706\n",
      "Epoch 88/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5741 - mae: 0.9139 - val_loss: 1.5612 - val_mae: 0.8452\n",
      "Epoch 89/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6056 - mae: 0.9017 - val_loss: 1.5309 - val_mae: 0.8438\n",
      "Epoch 90/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5783 - mae: 0.8974 - val_loss: 1.4979 - val_mae: 0.8454\n",
      "Epoch 91/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6501 - mae: 0.9164 - val_loss: 1.4480 - val_mae: 0.8071\n",
      "Epoch 92/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5316 - mae: 0.8717 - val_loss: 1.4855 - val_mae: 0.8607\n",
      "Epoch 93/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6529 - mae: 0.9375 - val_loss: 1.4264 - val_mae: 0.7759\n",
      "Epoch 94/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6654 - mae: 0.9324 - val_loss: 1.7305 - val_mae: 0.9666\n",
      "Epoch 95/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.9404 - mae: 1.0307 - val_loss: 1.5797 - val_mae: 0.8824\n",
      "Epoch 96/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6366 - mae: 0.8971 - val_loss: 1.4604 - val_mae: 0.7894\n",
      "Epoch 97/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.5770 - mae: 0.8801 - val_loss: 1.4174 - val_mae: 0.8122\n",
      "Epoch 98/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.6799 - mae: 0.9393 - val_loss: 1.4121 - val_mae: 0.7918\n",
      "Epoch 99/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5620 - mae: 0.8703 - val_loss: 1.4060 - val_mae: 0.8112\n",
      "Epoch 100/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 1.5258 - mae: 0.8666 - val_loss: 1.6574 - val_mae: 0.9167\n",
      "Total RAM usage: 585.04 MB\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import psutil\n",
    "\n",
    "# Define LSTM model\n",
    "# Layers 128, 64 and 32 are chosen by parameter tuning\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, input_shape=(sequence_length, 1), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.1),\n",
    "    LSTM(64, activation='relu', return_sequences=True, kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.1),\n",
    "    LSTM(32, activation='relu', return_sequences=False, kernel_regularizer=l2(0.001)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Finding RAM usage\n",
    "ram_usage = psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "print(f\"Total RAM usage: {ram_usage:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
      "Mean Absolute Error (MAE): 0.8285638093948364\n",
      "Accuracy: 0.48626373626373626\n"
     ]
    }
   ],
   "source": [
    "# Calculating the MAE and Accuracy\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "correct = []\n",
    "incorrect = []\n",
    "pred = np.round(pred).astype(int)\n",
    "for i in range(len(y_val)):\n",
    "    yt_event = y_val[i]\n",
    "    pred_event = pred[i]\n",
    "\n",
    "    yt_event = yt_event.reshape(1,)\n",
    "    pred_event = pred_event.reshape(1,)\n",
    "\n",
    "    if np.abs(yt_event - pred_event) < 1:\n",
    "        correct.append(y_val[i])\n",
    "    else:\n",
    "        incorrect.append(y_val[i])\n",
    "\n",
    "\n",
    "accuracy = len(correct) / len(y_val)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model for future use\n",
    "model_path = './trained_models'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "model.save(f'{model_path}/lstm_id_alone_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./scalers/scaler_lstm_id.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the scaler\n",
    "import joblib\n",
    "\n",
    "scaler_path = './scalers'\n",
    "if not os.path.exists(scaler_path):\n",
    "    os.makedirs(scaler_path)\n",
    "joblib.dump(scaler, f'{scaler_path}/scaler_lstm_id.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
