{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM ID ALONE MODEL\n",
    "# This model is used to predict the next ID of a sequence of IDs\n",
    "# The model is trained on the ID alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import get_paths, read_traces, read_json, mapint2var, is_consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal base path: ./trace_data/theft_protection/single_thread/version_3/normal\n",
      "Faulty base path: ./trace_data/theft_protection/single_thread/version_3/faulty_data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CODE = 'theft_protection'               ### application (code) theft_protection, mamba2, lora_ducy\n",
    "BEHAVIOUR_FAULTY = 'faulty_data'        ### normal, faulty_data\n",
    "BEHAVIOUR_NORMAL = 'normal'             ### normal, faulty_data\n",
    "THREAD = 'single'                       ### single, multi\n",
    "VER = 3                                 ### format of data collection\n",
    "\n",
    "base_dir = './trace_data'              ### can be replaced with 'csv', 'exe_plot', 'histogram'\n",
    "normalbase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_NORMAL}'\n",
    "faultybase_path = base_dir+f'/{CODE}/{THREAD}_thread/version_{VER}/{BEHAVIOUR_FAULTY}'\n",
    "\n",
    "print(\"Normal base path:\", normalbase_path)\n",
    "print(\"Faulty base path:\", faultybase_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_base_path = os.path.join(normalbase_path, 'train_data')\n",
    "train_data_path = [os.path.join(train_base_path, x) for x in os.listdir(train_base_path)]\n",
    "train_varlist_path = [os.path.join(normalbase_path, x) for x in os.listdir(normalbase_path) if 'varlist' in x]\n",
    "\n",
    "######### get paths #######################\n",
    "paths_log, paths_traces, varlist_path, paths_label = get_paths(faultybase_path)\n",
    "\n",
    "train_data_path = [x for x in train_data_path if '.DS_Store' not in x]\n",
    "train_varlist_path = [x for x in train_varlist_path if '.DS_Store' not in x]\n",
    "paths_log = [x for x in paths_log if '.DS_Store' not in x]\n",
    "paths_traces = [x for x in paths_traces if '.DS_Store' not in x]\n",
    "varlist_path = [x for x in varlist_path if '.DS_Store' not in x]\n",
    "paths_label = [x for x in paths_label if '.DS_Store' not in x]\n",
    "\n",
    "paths_log.sort()\n",
    "paths_traces.sort()\n",
    "varlist_path.sort()\n",
    "paths_label.sort()\n",
    "\n",
    "test_data_path = paths_traces\n",
    "test_label_path = paths_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varlist 1 is consistent with varlist 0\n",
      "varlist 2 is consistent with varlist 0\n",
      "varlist 3 is consistent with varlist 0\n"
     ]
    }
   ],
   "source": [
    "# Check consistency\n",
    "if VER == 3:\n",
    "    check_con, _ = is_consistent([train_varlist_path[0]] + varlist_path)\n",
    "    if check_con:\n",
    "        to_number = read_json(varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "    else:\n",
    "        to_number = read_json(train_varlist_path[0])\n",
    "        from_number = mapint2var(to_number)\n",
    "\n",
    "sorted_keys = list(from_number.keys())\n",
    "sorted_keys.sort()\n",
    "var_list = [from_number[key] for key in sorted_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15], [13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14], [9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10], [7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8, 9, 6, 7, 8, 9, 6, 7, 8, 9, 10, 11, 12, 6, 7, 8, 9, 13, 14, 15, 6, 7, 8]]\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "def load_data(file_paths):\n",
    "    data = []\n",
    "    for file in file_paths:\n",
    "        traces = read_traces(file)\n",
    "        if isinstance(traces, list):\n",
    "            id_sequence = [int(trace[0]) for trace in traces]\n",
    "            data.append(id_sequence)\n",
    "    return data\n",
    "\n",
    "train_data = load_data(train_data_path)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: shape = (110,)\n",
      "Dataset 2: shape = (750,)\n",
      "Dataset 3: shape = (750,)\n",
      "Dataset 4: shape = (250,)\n"
     ]
    }
   ],
   "source": [
    "for idx, td in enumerate(train_data):\n",
    "    td_array = np.array(td)  \n",
    "    print(f\"Dataset {idx + 1}: shape = {td_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare LSTM training data\n",
    "sequence_length = 10       # Already tuned parameter value\n",
    "X_train, y_train = [], []\n",
    "for single_file_data in train_data:\n",
    "    for i in range(len(single_file_data) - sequence_length):\n",
    "        X_train.append(single_file_data[i:i + sequence_length])\n",
    "        y_train.append(single_file_data[i + sequence_length])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "X_train_new = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_new = X_val.reshape(-1, X_val.shape[-1])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_new)\n",
    "X_val_scaled = scaler.transform(X_val_new)\n",
    "\n",
    "X_train = X_train_scaled.reshape(X_train.shape)\n",
    "X_val = X_val_scaled.reshape(X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 80.5211 - mae: 8.6197 - val_loss: 78.1659 - val_mae: 8.4539\n",
      "Epoch 2/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 70.2142 - mae: 7.9027 - val_loss: 19.2465 - val_mae: 3.4670\n",
      "Epoch 3/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 19.7069 - mae: 3.3671 - val_loss: 15.8669 - val_mae: 3.2058\n",
      "Epoch 4/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 15.3330 - mae: 2.8759 - val_loss: 13.3207 - val_mae: 2.9634\n",
      "Epoch 5/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 12.0691 - mae: 2.6814 - val_loss: 10.8544 - val_mae: 2.3442\n",
      "Epoch 6/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 10.7421 - mae: 2.4258 - val_loss: 10.0255 - val_mae: 2.2353\n",
      "Epoch 7/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 9.8579 - mae: 2.2001 - val_loss: 8.7155 - val_mae: 2.2172\n",
      "Epoch 8/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 8.6823 - mae: 2.1396 - val_loss: 7.8526 - val_mae: 2.1039\n",
      "Epoch 9/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 7.4884 - mae: 1.9937 - val_loss: 6.8931 - val_mae: 1.9029\n",
      "Epoch 10/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.7948 - mae: 1.9365 - val_loss: 6.2969 - val_mae: 1.8573\n",
      "Epoch 11/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 7.1532 - mae: 2.0442 - val_loss: 6.5538 - val_mae: 1.8679\n",
      "Epoch 12/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 6.6641 - mae: 1.9384 - val_loss: 6.6811 - val_mae: 1.8678\n",
      "Epoch 13/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.6356 - mae: 1.8913 - val_loss: 6.4609 - val_mae: 1.9883\n",
      "Epoch 14/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.4080 - mae: 1.9282 - val_loss: 6.3734 - val_mae: 2.0119\n",
      "Epoch 15/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.4580 - mae: 2.0066 - val_loss: 5.9576 - val_mae: 1.8912\n",
      "Epoch 16/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.0862 - mae: 1.9257 - val_loss: 6.0979 - val_mae: 1.8491\n",
      "Epoch 17/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 6.3598 - mae: 1.9771 - val_loss: 5.9795 - val_mae: 2.0341\n",
      "Epoch 18/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 6.4467 - mae: 2.0629 - val_loss: 5.7505 - val_mae: 1.8822\n",
      "Epoch 19/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.9893 - mae: 1.9063 - val_loss: 5.9716 - val_mae: 1.8330\n",
      "Epoch 20/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 6.0177 - mae: 1.9112 - val_loss: 5.8095 - val_mae: 1.8346\n",
      "Epoch 21/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 6.1832 - mae: 1.9355 - val_loss: 5.6825 - val_mae: 1.8432\n",
      "Epoch 22/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 6.1355 - mae: 1.9741 - val_loss: 5.7308 - val_mae: 1.9664\n",
      "Epoch 23/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.0497 - mae: 1.9834 - val_loss: 5.6827 - val_mae: 1.9436\n",
      "Epoch 24/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.2377 - mae: 1.9937 - val_loss: 5.7310 - val_mae: 1.9637\n",
      "Epoch 25/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.8679 - mae: 1.9172 - val_loss: 5.5948 - val_mae: 1.9229\n",
      "Epoch 26/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.7364 - mae: 1.8704 - val_loss: 5.5838 - val_mae: 1.8623\n",
      "Epoch 27/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.0312 - mae: 1.9318 - val_loss: 5.7988 - val_mae: 1.8154\n",
      "Epoch 28/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.9882 - mae: 1.9222 - val_loss: 5.6606 - val_mae: 1.8281\n",
      "Epoch 29/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.8956 - mae: 1.9049 - val_loss: 5.5067 - val_mae: 1.8662\n",
      "Epoch 30/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 6.0746 - mae: 1.9686 - val_loss: 5.4116 - val_mae: 1.8723\n",
      "Epoch 31/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.2175 - mae: 1.8125 - val_loss: 5.5785 - val_mae: 1.9288\n",
      "Epoch 32/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.7818 - mae: 1.9046 - val_loss: 5.4375 - val_mae: 1.8689\n",
      "Epoch 33/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.2041 - mae: 1.9932 - val_loss: 5.5126 - val_mae: 1.8249\n",
      "Epoch 34/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.6497 - mae: 1.8876 - val_loss: 5.4689 - val_mae: 1.8481\n",
      "Epoch 35/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.7479 - mae: 1.8723 - val_loss: 5.4755 - val_mae: 1.8953\n",
      "Epoch 36/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.7924 - mae: 1.9352 - val_loss: 5.4165 - val_mae: 1.8613\n",
      "Epoch 37/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.7447 - mae: 1.8628 - val_loss: 5.7188 - val_mae: 1.7995\n",
      "Epoch 38/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.8090 - mae: 1.8658 - val_loss: 5.4663 - val_mae: 1.8508\n",
      "Epoch 39/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.7615 - mae: 1.8878 - val_loss: 5.4256 - val_mae: 1.8869\n",
      "Epoch 40/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.7962 - mae: 1.9197 - val_loss: 5.3303 - val_mae: 1.8659\n",
      "Epoch 41/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.4994 - mae: 1.8538 - val_loss: 5.3571 - val_mae: 1.8263\n",
      "Epoch 42/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.9005 - mae: 1.9078 - val_loss: 5.3245 - val_mae: 1.8414\n",
      "Epoch 43/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 5.6328 - mae: 1.8843 - val_loss: 5.2775 - val_mae: 1.8547\n",
      "Epoch 44/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - loss: 5.6795 - mae: 1.8757 - val_loss: 5.3049 - val_mae: 1.8229\n",
      "Epoch 45/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.8743 - mae: 1.9204 - val_loss: 5.3202 - val_mae: 1.8280\n",
      "Epoch 46/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.6743 - mae: 1.9012 - val_loss: 5.3234 - val_mae: 1.8405\n",
      "Epoch 47/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - loss: 5.9457 - mae: 1.9344 - val_loss: 5.2985 - val_mae: 1.8484\n",
      "Epoch 48/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.5704 - mae: 1.8633 - val_loss: 5.2765 - val_mae: 1.8252\n",
      "Epoch 49/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.7832 - mae: 1.9033 - val_loss: 5.1939 - val_mae: 1.8202\n",
      "Epoch 50/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.3749 - mae: 1.8471 - val_loss: 5.3446 - val_mae: 1.8708\n",
      "Epoch 51/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.7672 - mae: 1.8992 - val_loss: 5.2241 - val_mae: 1.8337\n",
      "Epoch 52/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.6364 - mae: 1.8813 - val_loss: 5.2918 - val_mae: 1.7979\n",
      "Epoch 53/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.3740 - mae: 1.8315 - val_loss: 5.2349 - val_mae: 1.8235\n",
      "Epoch 54/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.4291 - mae: 1.8268 - val_loss: 5.1155 - val_mae: 1.8011\n",
      "Epoch 55/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.6648 - mae: 1.8880 - val_loss: 5.1032 - val_mae: 1.8167\n",
      "Epoch 56/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.4974 - mae: 1.8577 - val_loss: 5.1895 - val_mae: 1.7572\n",
      "Epoch 57/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.6063 - mae: 1.8745 - val_loss: 5.2232 - val_mae: 1.8009\n",
      "Epoch 58/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.7090 - mae: 1.8732 - val_loss: 5.4023 - val_mae: 1.8843\n",
      "Epoch 59/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.3836 - mae: 1.8407 - val_loss: 5.1997 - val_mae: 1.8359\n",
      "Epoch 60/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.4204 - mae: 1.8233 - val_loss: 5.1543 - val_mae: 1.7558\n",
      "Epoch 61/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.4213 - mae: 1.8335 - val_loss: 5.4172 - val_mae: 1.8856\n",
      "Epoch 62/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.6357 - mae: 1.9074 - val_loss: 5.6480 - val_mae: 1.9750\n",
      "Epoch 63/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.6631 - mae: 1.9015 - val_loss: 5.3684 - val_mae: 1.8411\n",
      "Epoch 64/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.4764 - mae: 1.8441 - val_loss: 5.1542 - val_mae: 1.7727\n",
      "Epoch 65/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.0355 - mae: 1.7676 - val_loss: 4.7936 - val_mae: 1.7450\n",
      "Epoch 66/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.5003 - mae: 1.8700 - val_loss: 4.8866 - val_mae: 1.7027\n",
      "Epoch 67/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.3145 - mae: 1.7983 - val_loss: 4.5532 - val_mae: 1.6914\n",
      "Epoch 68/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.3659 - mae: 1.8111 - val_loss: 5.4147 - val_mae: 1.8438\n",
      "Epoch 69/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.4830 - mae: 1.8325 - val_loss: 5.4397 - val_mae: 1.8059\n",
      "Epoch 70/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 6.1175 - mae: 1.9454 - val_loss: 5.3516 - val_mae: 1.7712\n",
      "Epoch 71/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.3007 - mae: 1.7838 - val_loss: 5.4501 - val_mae: 1.8782\n",
      "Epoch 72/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.5600 - mae: 1.8630 - val_loss: 5.2704 - val_mae: 1.8341\n",
      "Epoch 73/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.6699 - mae: 1.9106 - val_loss: 5.1446 - val_mae: 1.8270\n",
      "Epoch 74/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.5851 - mae: 1.8786 - val_loss: 5.4843 - val_mae: 1.9239\n",
      "Epoch 75/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - loss: 5.5501 - mae: 1.8651 - val_loss: 5.2590 - val_mae: 1.8425\n",
      "Epoch 76/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 5.7912 - mae: 1.9133 - val_loss: 5.1570 - val_mae: 1.7810\n",
      "Epoch 77/100\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 5.7557 - mae: 1.9065 - val_loss: 5.1872 - val_mae: 1.7620\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# Define LSTM model\n",
    "# Layers 128, 64 and 32 are chosen by parameter tuning\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', return_sequences=True, input_shape=(sequence_length, 1), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.1),\n",
    "    LSTM(64, activation='relu', return_sequences=True, kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.1),\n",
    "    LSTM(32, activation='relu', return_sequences=False, kernel_regularizer=l2(0.001)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='mse',\n",
    "              metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n",
      "Mean Absolute Error (MAE): 1.5863466262817383\n",
      "Accuracy: 0.25280898876404495\n"
     ]
    }
   ],
   "source": [
    "# Calculating the MAE and Accuracy\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "\n",
    "correct = []\n",
    "incorrect = []\n",
    "pred = np.round(pred).astype(int)\n",
    "for i in range(len(y_val)):\n",
    "    yt_event = y_val[i]\n",
    "    pred_event = pred[i]\n",
    "\n",
    "    yt_event = yt_event.reshape(1,)\n",
    "    pred_event = pred_event.reshape(1,)\n",
    "\n",
    "    if np.abs(yt_event - pred_event) < 1:\n",
    "        correct.append(y_val[i])\n",
    "    else:\n",
    "        incorrect.append(y_val[i])\n",
    "\n",
    "\n",
    "accuracy = len(correct) / len(y_val)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
